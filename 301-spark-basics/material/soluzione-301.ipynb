{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7243a-b7d3-4e47-b3ec-49cafdebada5",
   "metadata": {},
   "source": [
    "# 301 Spark basics\n",
    "\n",
    "The goal of this lab is to get familiar with Spark programming.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "\n",
    "## 301-2 Running a sample Spark job\n",
    "\n",
    "Goal: calculate the average temperature for every month; dataset is ```weather-sample1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9323446c-bdfe-4eff-b87a-ce57f7c840b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3400bf68094ca88c9c8aa56ec2704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1696445664495_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1696445664495_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-3JVE583OPXE10\n",
       "\" application-id=\"application_1696445664495_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-84-119.ec2.internal:8042/node/containerlogs/container_1696445664495_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-dcohen\n",
      "rddWeather: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-dcohen/datasets/weather-sample1.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-dcohen\"\n",
    "\n",
    "val rddWeather = sc.textFile(\"s3a://\"+bucketname+\"/datasets/weather-sample1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a5830c-1cd8-4f3a-bfc5-f91e9aa8d87f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79fab6055db4e64b126c98ad575618f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parseWeatherLine: (line: String)(String, Double)\n",
      "rddWeatherKv: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[2] at map at <console>:27\n",
      "rddTempDataPerMonth: org.apache.spark.rdd.RDD[(String, (Double, Double))] = ShuffledRDD[3] at aggregateByKey at <console>:27\n",
      "rddAvgTempPerMonth: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[4] at map at <console>:25\n",
      "rddCached: org.apache.spark.rdd.RDD[(String, Double)] = CoalescedRDD[8] at coalesce at <console>:24\n"
     ]
    }
   ],
   "source": [
    "// \n",
    "def parseWeatherLine(line:String):(String,Double) = {\n",
    "  val year = line.substring(15,19)\n",
    "  val month = line.substring(19,21)\n",
    "  val day = line.substring(21,23)\n",
    "  var temp = line.substring(87,92).toInt\n",
    "  (month, temp/10)\n",
    "}\n",
    "\n",
    "// Parse records (for each item apply the function key-value)\n",
    "// In order to enable the operation to allow aggregration you need that rdd is structure in kv\n",
    "val rddWeatherKv = rddWeather.map(x => parseWeatherLine(x))\n",
    "// Aggregate by key (i.e., month) to compute the sum and the count of temperature values\n",
    "// specify two combinitation before shuffling and after\n",
    "// The behaviour is the same in this case but is specified differently\n",
    "// Its important because spark always try to do some combining even with reducebykey always try to adopt this to the map size.\n",
    "val rddTempDataPerMonth = rddWeatherKv.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))\n",
    "// Calculate the average temperature in each record\n",
    "// Map operation wherete the is the touple divide by the sum and count\n",
    "val rddAvgTempPerMonth = rddTempDataPerMonth.map({case(k,v) => (k, v._1/v._2)})\n",
    "// Sort, coalesce and cache the result (because it is used twice)\n",
    "val rddCached = rddAvgTempPerMonth.sortByKey().coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9621e29d-4c97-4779-b7d1-e5126bce5d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e017a5f7f8da43e89fb9c984cca9664c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res14: Array[(String, Double)] = Array((01,29.764781644286497), (02,52.831468961278425), (03,49.43499927074724), (04,61.3592872169286), (05,55.82656), (06,55.45816479125297), (07,86.90952392350223), (08,79.250958082407), (09,80.51662117371808), (10,106.26454490168254), (11,113.49704495968224), (12,63.9184413544602))\n"
     ]
    }
   ],
   "source": [
    "// Show all the records\n",
    "rddCached.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50d2419-c6a0-4616-be1c-c24f09653107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be94508662024ee080e5de376a78adaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory s3a://unibo-bd2324-dcohen/spark/301-2 already exists\n",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n",
      "  ... 51 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddCached.saveAsTextFile(\"s3a://\"+bucketname+\"/spark/301-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b8fdd-ef59-4e64-b749-1d9ae50260bc",
   "metadata": {},
   "source": [
    "## 301-3 Spark warm-up\n",
    "\n",
    "Load the ```capra``` and ```divinacommedia``` datasets and try the following actions:\n",
    "- Show their content (```collect```)\n",
    "- Count their rows (```count```)\n",
    "- Split phrases into words (```map``` or ```flatMap```; what’s the difference?)\n",
    "- Check the results (remember: evaluation is lazy)\n",
    "- Try the ```toDebugString``` function to check the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52649088-9a57-437b-9127-0d29984b4e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5d027b815446189b17ae419a056abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapra: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-dcohen/datasets/capra.txt MapPartitionsRDD[11] at textFile at <console>:24\n",
      "rddDC: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-dcohen/datasets/divinacommedia.txt MapPartitionsRDD[13] at textFile at <console>:24\n"
     ]
    }
   ],
   "source": [
    "val rddCapra = sc.textFile(\"s3a://\"+bucketname+\"/datasets/capra.txt\")\n",
    "val rddDC = sc.textFile(\"s3a://\"+bucketname+\"/datasets/divinacommedia.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb57f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da585e5d7c4e65ada03c69d4e64f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res16: Array[String] = Array(sopra la panca la capra campa, sotto la panca la capra crepa)\n",
      "res17: Array[String] = Array(LA DIVINA COMMEDIA, di Dante Alighieri, INFERNO, \"\", \"\", \"\", Inferno: Canto I, \"\", \"  Nel mezzo del cammin di nostra vita\", mi ritrovai per una selva oscura, ch? la diritta via era smarrita., \"  Ahi quanto a dir qual era ? cosa dura\", esta selva selvaggia e aspra e forte, che nel pensier rinova la paura!, \"  Tant'? amara che poco ? pi? morte;\", ma per trattar del ben ch'i' vi trovai,, dir? de l'altre cose ch'i' v'ho scorte., \"  Io non so ben ridir com'i' v'intrai,\", tant'era pien di sonno a quel punto, che la verace via abbandonai., \"  Ma poi ch'i' fui al pi? d'un colle giunto,\", l? dove terminava quella valle, che m'avea di paura il cor compunto,, \"  guardai in alto, e vidi le sue spalle\", vestite gi? de' raggi del pianeta, che mena dritto altrui per ogne c...\n"
     ]
    }
   ],
   "source": [
    "rddCapra.collect()\n",
    "rddDC.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ef9d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d26ce4d9814bc5b5da14df37eb51f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: Long = 2\n",
      "res19: Long = 14753\n"
     ]
    }
   ],
   "source": [
    "rddCapra.count()\n",
    "rddDC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d315b641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984d23560ee945d7b5102c38c52cc3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapraSplitted: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at flatMap at <console>:23\n",
      "res29: Array[String] = Array(sopra, la, panca, la, capra, campa, sotto, la, panca, la, capra, crepa)\n"
     ]
    }
   ],
   "source": [
    "val rddCapraSplitted = rddCapra.flatMap(_.split(\" \"))\n",
    "// rddCapraSplitted.toDebugString\n",
    "rddCapraSplitted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b04373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055e7831ae40431481c578658fb161c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddDCSplitted: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at flatMap at <console>:23\n",
      "res31: Array[String] = Array(LA, DIVINA, COMMEDIA, di, Dante, Alighieri, INFERNO, \"\", \"\", \"\", Inferno:, Canto, I, \"\", \"\", \"\", Nel, mezzo, del, cammin, di, nostra, vita, mi, ritrovai, per, una, selva, oscura, ch?, la, diritta, via, era, smarrita., \"\", \"\", Ahi, quanto, a, dir, qual, era, ?, cosa, dura, esta, selva, selvaggia, e, aspra, e, forte, che, nel, pensier, rinova, la, paura!, \"\", \"\", Tant'?, amara, che, poco, ?, pi?, morte;, ma, per, trattar, del, ben, ch'i', vi, trovai,, dir?, de, l'altre, cose, ch'i', v'ho, scorte., \"\", \"\", Io, non, so, ben, ridir, com'i', v'intrai,, tant'era, pien, di, sonno, a, quel, punto, che, la, verace, via, abbandonai., \"\", \"\", Ma, poi, ch'i', fui, al, pi?, d'un, colle, giunto,, l?, dove, terminava, quella, valle, che, m'avea, di, paura, il, cor, compunto...\n"
     ]
    }
   ],
   "source": [
    "val rddDCSplitted = rddDC.flatMap(elem => elem.split(\" \"))\n",
    "// rddDCSplitted.toDebugString\n",
    "rddDCSplitted.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d9363-3ce0-411b-a826-ee3cceb935a1",
   "metadata": {},
   "source": [
    "## 301-4 From MapReduce to Spark\n",
    "\n",
    "Reproduce on Spark the exercises seen on Hadoop MapReduce on the capra and divinacommedia datasets.\n",
    "\n",
    "- Jobs:\n",
    "  - Count the number of occurrences of each word\n",
    "    - Result: (sopra, 1), (la, 4), …\n",
    "  - Count the number of occurrences of words of given lengths\n",
    "    - Result: (2, 4), (5, 8)\n",
    "  - Count the average length of words given their first letter (hint: check the example in 301-1)\n",
    "    - Result: (s, 5), (l, 2), …\n",
    "  - Return the list of offsets of each words\n",
    "    - Result: (sopra, (0)), (la, (0, 1)), ...\n",
    "- How does Spark compare with respect to MapReduce? (performance, ease of use)\n",
    "- How is the output sorted? How can you sort by value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd0b306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f6e8b3c39b4342b3c7a1bde282607d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapraCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[37] at reduceByKey at <console>:28\n",
      "res47: String =\n",
      "(2) ShuffledRDD[37] at reduceByKey at <console>:28 []\n",
      " +-(2) MapPartitionsRDD[36] at map at <console>:27 []\n",
      "    |  MapPartitionsRDD[35] at flatMap at <console>:26 []\n",
      "    |  s3a://unibo-bd2324-dcohen/datasets/capra.txt MapPartitionsRDD[11] at textFile at <console>:24 []\n",
      "    |  s3a://unibo-bd2324-dcohen/datasets/capra.txt HadoopRDD[10] at textFile at <console>:24 []\n"
     ]
    }
   ],
   "source": [
    "// Occurrences Count\n",
    "\n",
    "val rddCapraCount = rddCapra.\n",
    "   flatMap( _.split(\" \") ).\n",
    "   map((_,1)).\n",
    "   reduceByKey((x,y)=>x+y)\n",
    "\n",
    "rddCapraCount.collect()\n",
    "// rddCapraCount.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367b6aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224a2c83a29448c1915648273ad4d88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapraOccurrences: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[51] at reduceByKey at <console>:28\n",
      "res70: Array[(Int, Int)] = Array((2,4), (5,8))\n"
     ]
    }
   ],
   "source": [
    "// Word length count\n",
    "\n",
    "val rddCapraOccurrences = rddCapra.\n",
    "    flatMap( _.split(\" \") ).\n",
    "    map(word => (word.length, 1)).\n",
    "    reduceByKey((x,y) => x + y)\n",
    "\n",
    "rddCapraOccurrences.collect()\n",
    "// rddCapraOccurrences.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06a01566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1890d31ba6694648b7e8c3d99e612ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapraAvgLenMap: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[63] at map at <console>:27\n",
      "res92: Array[(String, Int)] = Array((s,5), (l,2), (p,5), (l,2), (c,5), (c,5), (s,5), (l,2), (p,5), (l,2), (c,5), (c,5))\n",
      "rddCapraAvgLenReduce: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[64] at aggregateByKey at <console>:26\n",
      "res96: Array[(String, (Int, Int))] = Array((p,(10,2)), (l,(8,4)), (s,(10,2)), (c,(20,4)))\n",
      "rddCapraAvgLenFinal: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[65] at mapValues at <console>:26\n",
      "res100: Array[(String, Int)] = Array((p,5), (l,2), (s,5), (c,5))\n"
     ]
    }
   ],
   "source": [
    "// Average word Length by initial\n",
    "\n",
    "val rddCapraAvgLenMap = rddCapra.flatMap( _.split(\" \") ).\n",
    "    filter ( _.length > 0).\n",
    "    map (w => (w.take(1),w.length))\n",
    "\n",
    "rddCapraAvgLenMap.collect()\n",
    "// rddCapraAvgLength.toDebugString\n",
    "\n",
    "val rddCapraAvgLenReduce = rddCapraAvgLenMap.\n",
    "    aggregateByKey((0,0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))  //reduce\n",
    "\n",
    "rddCapraAvgLenReduce.collect()\n",
    "// rddCapraAvgLength.toDebugString\n",
    "\n",
    "val rddCapraAvgLenFinal = rddCapraAvgLenReduce.\n",
    "    mapValues(v => v._1/v._2)\n",
    "\n",
    "rddCapraAvgLenFinal.collect()\n",
    "// rddCapraAvgLength.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba78022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7563feaa7eeb4962a4c5acf22ffad10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapraMap: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[70] at zipWithIndex at <console>:26\n",
      "rddGroup: org.apache.spark.rdd.RDD[(String, Iterable[Long])] = ShuffledRDD[71] at groupByKey at <console>:27\n",
      "res114: Array[(String, Iterable[Long])] = Array((campa,CompactBuffer(5)), (la,CompactBuffer(1, 3, 7, 9)), (panca,CompactBuffer(2, 8)), (sotto,CompactBuffer(6)), (crepa,CompactBuffer(11)), (sopra,CompactBuffer(0)), (capra,CompactBuffer(4, 10)))\n"
     ]
    }
   ],
   "source": [
    "// Inverted index (word-based offset)\n",
    "\n",
    "val rddCapraMap = rddCapra.\n",
    "    flatMap( _.split(\" \") ).\n",
    "    zipWithIndex()\n",
    "\n",
    "rddMap.collect()\n",
    "// rddMap.toDebugString\n",
    "\n",
    "// CompactBuffer isan alternative to ArrayBuffer that \n",
    "// results in better performance because it allocates less memory.\n",
    "\n",
    "val rddGroup = rddMap.groupByKey()\n",
    "rddGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f70dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed45b4d50a2464ba02d5f0e996025ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddMap: org.apache.spark.rdd.RDD[(String, Long)] = MapPartitionsRDD[78] at distinct at <console>:27\n",
      "res116: Array[(String, Long)] = Array((la,0), (capra,0), (la,1), (sotto,1), (sopra,0), (capra,1), (panca,1), (crepa,1), (campa,0), (panca,0))\n",
      "rddGroup: org.apache.spark.rdd.RDD[(String, Iterable[Long])] = ShuffledRDD[79] at groupByKey at <console>:24\n",
      "res118: Array[(String, Iterable[Long])] = Array((campa,CompactBuffer(0)), (la,CompactBuffer(0, 1)), (panca,CompactBuffer(1, 0)), (sotto,CompactBuffer(1)), (crepa,CompactBuffer(1)), (sopra,CompactBuffer(0)), (capra,CompactBuffer(0, 1)))\n"
     ]
    }
   ],
   "source": [
    "// Inverted index (sentence-based offset) alternative\n",
    "val rddMap = rddCapra.zipWithIndex().\n",
    "    map({case (k,v)=>(v,k)}).\n",
    "    flatMapValues( x => x.split(\" \") ).\n",
    "    map({case (k,v)=>(v,k)}).\n",
    "    distinct()\n",
    "rddMap.collect()\n",
    "\n",
    "val rddGroup = rddMap.groupByKey()\n",
    "rddGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6610445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59bfa63ae484676887541ab8811ddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:30: error: constructor cannot be instantiated to expected type;\n",
      " found   : (T1, T2)\n",
      " required: String\n",
      "       rddCapra.map({case(k,v) => (v,k)}).sortByKey()\n",
      "                         ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sort an RDD by key\n",
    "\n",
    "// rddCapra.sortByKey()\n",
    "\n",
    "// Sort an RDD by value\n",
    "\n",
    "// rddCapra.map({case(k,v) => (v,k)}).sortByKey()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
